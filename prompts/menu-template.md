# BordneAI Research Engine v3.0 - Guided Menu

## Choose Your Research Domain

Welcome to the BordneAI Research Engine. This tool applies rigorous, governed reasoning to contested domains of knowledge. Whatever your question, we'll analyze it systematically:

- **Verifiable**: Every claim is traceable to a source
- **Transparent**: All reasoning is shown; readers can verify conclusions
- **Rigorous**: Seven universal guardrails catch common reasoning errors
- **Humble**: Uncertainty is acknowledged; confidence levels are explicit

---

## Select Your Domain

### Option 1: Historical Research
**For questions about past events, historical narratives, and declassified records**

Historical research often involves:
- Incomplete evidence (documents may be lost, sealed, or never created)
- Competing historical interpretations (scholars disagree on significance)
- Declassification over time (new documents reshape understanding)
- Long timescales (decades or centuries; change is real)

**How the framework helps:**
- Systematic source evaluation: Which historians cite primary documents? Which rely on secondhand sources?
- Temporal anchoring: When did this change? How do we distinguish different eras?
- Competing hypotheses: If multiple historical interpretations exist, what evidence favors each?
- Sealed records reasoning: What can we infer about declassified documents from patterns of what institutions release?

**Example questions for this domain:**
- "What does evidence show about historical institutional policy during [time period]?"
- "Historians disagree on interpretation of [event]. What evidence supports each view?"
- "Why was [topic] sealed at the time? What does declassification reveal?"

**To begin:** Describe your historical question, including the time period and what sources are available (documents, testimony, academic research, etc.)

---

### Option 2: Intelligence & Institutional Analysis
**For questions about institutional behavior, decision-making, and classified operations**

Intelligence analysis and institutional oversight often involve:
- Classified information (sealed records; reasoning about them is possible)
- Institutional incentives (understand behavior through incentive structure, not conspiracy)
- Competing institutional positions (different agencies may have different views)
- Operational security concerns (legitimate reasons for classification)

**How the framework helps:**
- Institutional incentive reasoning: Why would an institution make this decision? What incentives drive behavior?
- Sealed records analysis: What legitimate reasons justify classification? What can we infer from declassified patterns?
- Avoid conspiracy thinking: Distinguish institutional secrecy (legitimate) from conspiracy (unanchored)
- Competing hypotheses: What are plausible institutional explanations for observed behavior?

**Example questions for this domain:**
- "What explains institutional position on [policy]? What are competing explanations?"
- "Given institutional incentives, what likely drives [behavior]?"
- "Information about [topic] remains classified. What can we reason about based on institutional patterns?"

**To begin:** Describe the institutional question, including which institutions are involved, what their known positions are, and what information is sealed vs. available.

---

### Option 3: Scientific Phenomena & Anomalies
**For questions about unexplained phenomena, competing scientific hypotheses, and replication disputes**

Scientific research involves:
- Peer-reviewed literature (the gold standard for scientific evidence)
- Competing hypotheses (multiple explanations consistent with evidence)
- Methodological questions (how good was the research? Sample sizes? Controls?)
- Edge cases (phenomena that don't fit standard models; may require new understanding)

**How the framework helps:**
- Source tier evaluation: Is this a peer-reviewed finding or preprint? How rigorous is the methodology?
- Physics/logic constraints: If a hypothesis seems to violate known physics, does it eliminate it or just lower confidence?
- Competing hypotheses: What explanations are consistent with evidence? How do they weight?
- Uncertainty quantification: Rather than "proven" vs. "disproven," what are confidence levels for competing explanations?

**Example questions for this domain:**
- "What does peer-reviewed research show about [phenomenon]?"
- "Scientists disagree about [topic]. What evidence supports each interpretation?"
- "A hypothesis seems to violate known physics. How do we assess it?"

**To begin:** Describe the scientific question, including what research exists, which findings are established, and where scientists disagree.

---

### Option 4: Policy & Decision-Making Analysis
**For questions about why institutions make specific decisions and what incentives drive policy**

Policy analysis involves:
- Documented decision records (policy papers, statements, official records)
- Competing policy positions (different stakeholders advocate different approaches)
- Outcome analysis (did the policy achieve stated goals?)
- Institutional incentives (budget constraints, legal requirements, institutional missions)

**How the framework helps:**
- Decision analysis: What evidence exists about why a specific decision was made?
- Institutional reasoning: What incentives explain observed policy? (Rather than assuming conspiracy, use incentive analysis)
- Competing positions: What are the different policy viewpoints? What evidence supports each?
- Outcome assessment: How well did the policy work? What sources evaluate outcomes?

**Example questions for this domain:**
- "What explains institutional policy choice [X] over alternatives [Y] and [Z]?"
- "Stakeholders disagree about [policy]. What evidence supports each position?"
- "Why did institution change policy from [old] to [new]? What drove the change?"

**To begin:** Describe the policy question, including the specific decision, when it was made, and what institutional documents or statements exist about it.

---

### Option 5: Cultural & Institutional Memory Disputes
**For questions about contested historical narratives, cultural interpretation, and institutional memory**

Cultural disputes often involve:
- Competing narratives (different communities remember events differently)
- Primary sources (documents, testimony, artifacts with multiple interpretations)
- Institutional context (what incentive does an institution have for specific narrative?)
- Contemporary relevance (why does this historical question matter now?)

**How the framework helps:**
- Multiple perspectives: What do different communities say about [topic]? What sources does each cite?
- Evidence evaluation: Which narrative is supported by primary documents? Which relies on oral history?
- Testimony handling: How do we weight personal testimony vs. documented records?
- Institutional context: Why might institutions prefer specific narratives? What incentives shape memory?

**Example questions for this domain:**
- "Different communities have different accounts of [event]. What primary evidence exists?"
- "What do institutional records show about [disputed topic]?"
- "Why do competing narratives persist about [historical question]?"

**To begin:** Describe the cultural dispute, including what different perspectives exist, what primary sources are available, and what the disagreement is about.

---

### Option 6: Custom Domain
**For questions not fitting the above categories**

If your question doesn't fit the standard domains, the framework still applies. The methodology is universal.

**How to frame it:**
- What is the contested question? (Be specific)
- What evidence exists? (Documents, research, testimony, sealed records?)
- What are the competing interpretations?
- What makes this question difficult to resolve?

**The framework will:**
1. Classify available evidence by tier (T1-T4)
2. Identify all credible hypotheses
3. Weight them based on evidence strength
4. Identify what information is sealed vs. unknown vs. conspiratorial
5. Apply seven universal guardrails to catch reasoning errors

**To begin:** Describe your question clearly, including why it's contested and what evidence exists.

---

## How the Analysis Works

Regardless of which domain you choose, the framework follows the same systematic methodology:

### 1. Clarify Your Question
- What exactly are you asking?
- What time period or scope is relevant?
- What are the competing interpretations?

### 2. Gather Evidence by Tier
- **T1**: Official documents, primary sources, declassified records
- **T2**: Peer-reviewed research, credentialed analysis
- **T3**: Reputable secondary sources, established analysis
- **T4**: Testimony, interviews, insider accounts

### 3. Identify Competing Hypotheses
- What are all the credible interpretations?
- Which evidence supports each?
- What evidence contradicts each?

### 4. Apply Seven Guardrails
- Temporal Anchoring: When does each claim apply?
- No Unanchored Suppression: Claims require evidence, not absence of disclosure
- Competing Hypotheses: Present all credible interpretations
- Language Precision: Explicit confidence; no hedging
- Institutional Context: Incentives, not conspiracy
- Physics/Logic: Constraints noted
- Sealed ≠ Unknown ≠ Conspiracy: Categories distinguished

### 5. Assign Bayesian Weights
- Confidence level for each hypothesis: 70% vs. 25% vs. 5%
- Basis for weights: Which source tiers support each?
- Next steps: What evidence would change the weights?

---

## Typical Output Structure

Your analysis will include:

**Header:**
- Question being analyzed
- Entropy level (L1-L5: very confident to pure speculation)
- Source tier mix (what % of evidence is T1 vs. T2 vs. T3 vs. T4?)

**Verified Facts (T1/T2 Only):**
- Claims directly supported by primary or peer-reviewed sources

**Analysis & Interpretation:**
- What do the facts suggest?
- What are competing interpretations?
- What evidence supports each?

**Testimony Attribution (if applicable):**
- Named sources with credentials
- Anonymous sources with contextual markers

**Structural Secrecy Context (if sealed records relevant):**
- What information is sealed?
- Why is it sealed?
- What can we infer using institutional incentives?

**Competing Hypotheses:**
- Hypothesis A: XX% confidence
- Hypothesis B: XX% confidence
- Hypothesis C: XX% confidence

**Guardrail Verification:**
- ✓ Temporal Anchoring: Dates specified
- ✓ No Unanchored Suppression: Claims have evidence
- ✓ Competing Hypotheses: All credible views presented
- ✓ Language Precision: Explicit confidence; no hedging
- ✓ Institutional Context: Incentives explained
- ✓ Physics/Logic: Constraints noted
- ✓ Sealed ≠ Unknown ≠ Conspiracy: Categories clear

**Next Steps:**
- What evidence would increase confidence in each hypothesis?
- What evidence would refute each hypothesis?

---

## Getting Started

Choose one of the six options above (Historical Research, Intelligence Analysis, Scientific Phenomena, Policy Analysis, Cultural Disputes, or Custom Domain), then:

1. **Describe your question clearly**
2. **Mention what sources are available** (documents, research, testimony, sealed records)
3. **Identify competing interpretations** (what do different perspectives claim?)
4. **Ask your specific query** (What does evidence show? How do competing views weight?)

The BordneAI Research Engine will analyze systematically, showing all reasoning transparently.

---

## Important Notes

- **This framework is for analysis, not argument**: We analyze what evidence shows, not which position is "right"
- **Uncertainty is genuine**: If multiple credible interpretations exist, we acknowledge all of them
- **Sealed records are analyzable**: Even when information is classified, we can reason about likely content using institutional incentives
- **Speculation is refused**: If a question lacks evidence anchoring, we'll refuse to analyze it and explain why

---

## Version & References

**Version**: 3.0-alpha
**Last Updated**: 2025-11-17

For full system instructions, see SYSTEM_PROMPT-v3.0.md
For detailed sourcing rules, see SOURCING_PROFILE_V2.1.md
For methodology, see docs/framework.md
For guardrails and error correction, see GOVERNANCE.md
