# Per-Domain Deep Dives

## Introduction

This document provides detailed methodology for each enabled domain, including sourcing priorities, reasoning constraints, and case studies showing how the governance framework operates in practice.

**Audience:** Domain specialists, policy makers, researchers seeking to understand how the BordneAI system approaches specific areas.

---

## Domain 1: Nuclear History & NC3

### Scope & Overview

This domain covers:
- U.S. nuclear weapons history, particularly Okinawa and Pacific deployments
- Nuclear Command & Control & Communications (NC3) systems and protocols
- Near-miss incidents, accidents, and institutional responses
- Declassified weapons policies, storage, and operational procedures
- Institutional secrecy norms around nuclear deterrence

### Why This Domain Matters

Nuclear weapons policy involves:
- Legitimate operational security (deterrent effectiveness requires some secrecy)
- Significant institutional sealing of records (NC3 details remain classified)
- Multiple institutional perspectives (military vs. civilian, U.S. vs. allied nations)
- Strong physics constraints (all claims must satisfy nuclear physics)

### T1 Priority Sources

**Primary Declassified Archives:**
- National Security Archive (George Washington University) — free, online; best source for declassified nuclear documents
- U.S. National Archives — Record Groups 218 (JCS), 220 (NSC), 59 (State Department)
- DoD Declassification Review Center — official release mechanism
- Los Alamos National Laboratory — declassified weapons research

**Specific Document Types to Prioritize:**
1. State Department cables marked "DECLASSIFIED" with declassification date
2. Joint Chiefs of Staff memoranda and assessments
3. Congressional hearing records (Senate Foreign Relations, Armed Services committees)
4. National Intelligence Estimates (declassified versions)
5. Military historical center publications (Naval History & Heritage Command, Air Force Historical Records)

**T1 Citation Standard for Nuclear Domain:**
```
[Source: Declassified [AGENCY] [DOCUMENT TYPE], [DATE],
Title: "[Exact Title]", available at [ARCHIVE] or
[URL to digital version]]
```

### T2 Priority Sources

**Peer-Reviewed History of Nuclear Weapons:**
- Norris, R. S. (2007). *Racing for the Bomb: General Leslie Groves, the Manhattan Project's Chief*. Steerforth Press.
- Rhodes, R. (1986). *The Making of the Atomic Bomb.* Simon & Schuster.
- Soddy, F. (1948). *Atomic Bombs: The Great Boomerang.* Hutchinson.
- Schlosser, E. (2013). *Command and Control: Nuclear Weapons, the Damascus Accident, and the Illusion of Safety.* Penguin Press.

**Physics & Engineering Analysis:**
- Van Denburg, C., et al. "Nuclear Command & Control Security Considerations" — peer-reviewed policy analysis
- Peer-reviewed papers on near-miss incidents, command authority questions, human factors in nuclear safety

**Domain-Specific CRS Reports:**
- Congressional Research Service reports on nuclear policy, proliferation, Pacific deployments (available at crsreports.congress.gov)

### Key Methodology: Nuclear History Case Study

#### Case: Okinawa Weapons Deployments (1950s–1970s)

**Question:** What is known about U.S. nuclear weapons presence in Okinawa?

**T1 Declassified Baseline:**

From National Security Archive declassified cables (1960s–1980s):
- ✅ Declassified in [year]: Specific weapons types (e.g., "nuclear-armed missiles") deployed to Okinawa [year]–[year]
- ✅ Declassified in [year]: Number of warheads approximately [range]
- ✅ Declassified in [year]: Storage procedures at [base] involved [specific protocols]
- ❌ Remains sealed: Current status (what's stored NOW)
- ❌ Remains sealed: Exact numbers for current deployments
- ❌ Remains sealed: Technical specifications of NC3 systems

**T2 Peer-Reviewed Analysis:**

Historian X (peer-reviewed article, 2015) analyzed declassified documents and concluded:
- Historical deployment confirmed
- Contextualizes within broader U.S. Pacific deterrence strategy
- Interprets institutional silence (non-disclosure to Okinawa residents) as deliberate policy
- Estimates current likely status based on broader patterns

Peer-reviewed policy paper (2018) on NC3 evolution notes:
- Historical NC3 protocols are partially declassified
- Modern protocols remain sealed for operational security
- Institutional incentive is clear: active system effectiveness requires secrecy

**T3 Historical Syntheses:**

Academic history (Rhodes, Schlosser, etc.) synthesizes declassified sources with oral histories to create narrative of U.S. nuclear posture in Pacific. Provides contextualization.

**Competing Hypotheses (Ranked by Evidence):**

1. **Weapons Remain (60% credible)**
   - Evidence: Institutional patterns suggest continuous deterrent presence; no declassified withdrawal announcements
   - Uncertainty: Current status sealed; could have been withdrawn quietly

2. **Weapons Withdrawn (30% credible)**
   - Evidence: Some weapons removed in [year] (declassified); broader U.S. disarmament trends
   - Uncertainty: Unclear if all withdrawn; current status unknown

3. **Status Unknown Due to Deliberate Secrecy (10% credible)**
   - Evidence: Institutional incentive to maintain uncertainty (deters adversaries from planning)
   - Uncertainty: Lowest credibility; most institutions favor declared or secret-but-known status

**Confidence Distribution:**
```
"Most likely scenario (60%): Weapons remain, presence kept deliberately
unconfirmed to maintain diplomatic ambiguity and deterrent value.

Alternative scenario (30%): Weapons withdrawn; current non-presence
maintained quietly to avoid political controversy.

Reasoning: Institutional incentive for secrecy is strong in both cases.
Declassification would likely show [specific facts], updating this
distribution. Confidence: 70% in analysis framework, 35% in specific
outcome."
```

**Sealed Records & Institutional Incentives:**

- **What's sealed:** Current weapons status, exact numbers, NC3 technical details
- **Why sealed:** Operational effectiveness depends on ambiguity/secrecy
  - If adversaries know weapons are present, deterrent value = credible threat
  - If adversaries know weapons are absent, deterrent value = zero
  - Maintaining strategic ambiguity is therefore institutional optimal strategy
- **What declassification would likely show:** Historical baseline, some policy decisions, possibly some modern inventory details
- **What declassification probably won't show:** Anything that undermines ongoing deterrent strategy

### Key Guardrails for Nuclear Domain

1. **Physics Constraints Binding:** All weapon capability claims must satisfy nuclear physics (criticality, yield, reliability)
2. **Temporal Anchoring Required:** All deployment claims tied to specific declassification events or dates
3. **NC3 Secrecy Legitimate:** Don't frame operational security needs as conspiracy
4. **Distinguish Confirmed/Plausible/Speculative:** Use tiers explicitly
5. **Respect Institutional Perspective:** Understand why DoD maintains certain positions

---

## Domain 2: Intelligence & Oversight

### Scope & Overview

This domain covers:
- Electoral history and voting patterns
- Declassified Intelligence Community Assessments (ICAs)
- Congressional oversight records and testimonies
- Intelligence activities disclosed via FOIA or declassification
- Institutional analysis of intelligence collection, analysis, and dissemination

### Why This Domain Matters

Intelligence domains involve:
- High-stakes policy implications (elections, foreign intervention, security decisions)
- Significant partisan interest (temptation to read evidence through partisan lens)
- Legitimate institutional secrecy (ongoing sources and methods, personnel protection)
- Contested interpretation (what declassified intelligence "really means")

### T1 Priority Sources

**Declassified Intelligence Products:**
- Intelligence Community Assessments (ICAs) — official T1 sources when declassified
- National Intelligence Estimates (NIEs) — declassified versions
- Congressional hearing records (Armed Services, Intelligence committees) — open session testimony
- Congressional Research Service reports on intelligence topics
- FOIA-released documents with verifiable declassification chains

**Specific Authorities:**
1. Declassified DNI (Director of National Intelligence) statements
2. Congressional Record testimony by intelligence officials
3. Inspector General reports (declassified portions)
4. Government Accountability Office (GAO) reports on intelligence oversight
5. ODNI.gov and CIA.gov official history pages (declassified materials)

**T1 Citation Standard for Intelligence Domain:**
```
[Source: Declassified Intelligence Community Assessment,
"[Title]", [DATE], available at [official DNI archive or
Congressional Record]]

or

[Source: Congressional Testimony, [Official Name], [Committee],
[Date], available at congress.gov]
```

### T2 Priority Sources

**Peer-Reviewed Analysis of Declassified Intelligence:**
- Academic journals: *Studies in Intelligence* (CIA-reviewed), *Intelligence & National Security*
- Books by intelligence historians (Amy Zegart, John Prados, etc.)
- Think tank analyses citing declassified products (Carnegie Endowment, Brookings, Council on Foreign Relations)
- Peer-reviewed political science analyzing electoral/intelligence institutional dynamics

**Domain-Specific CRS Reports:**
- CRS reports on election security, foreign interference, intelligence oversight
- Available at crsreports.congress.gov; official T2 sources

### Key Methodology: Intelligence Case Study

#### Case: 2016 Intelligence Community Assessment on Russian Election Interference

**Question:** What did declassified U.S. intelligence conclude about Russian interference in 2016 elections?

**T1 Declassified ICA (January 2017):**

Official IC Assessment states:
- ✅ Russian government directed certain interference operations
- ✅ Specific operations identified: (disinformation campaigns, hacking activities)
- ✅ IC judgement: [confidence level] on Russian intent to interfere
- ❌ Remains sealed/Disputed: Assessment of impact on election outcomes
- ❌ Remains sealed: Full technical details of interference methods (some classified for operational security)
- ❌ Remains sealed: Assessment of effectiveness

**T2 Peer-Reviewed Analyses:**

Political Science Study (2018): Analyzed declassified ICA and voting data
- Concludes: Russian interference occurred, impact on outcome unclear
- Method: Statistical analysis of voting patterns
- Confidence: Russian activity confirmed; causality attribution 45% credible

Intelligence Studies Journal (2019): Historian analyzes declassified sources
- Contextualization: How 2016 compares to historical precedent
- Interpretation: Why IC assessed Russian intent despite outcome uncertainty
- Competing explanations for lack of impact estimates

**T3 Historical & Journalistic Synthesis:**

- Major newspaper investigations (2017–2018): Synthesize declassified ICA with additional reporting
- Historical books: Place 2016 in context of U.S.–Russian intelligence competition

**Competing Hypotheses:**

1. **Russian Interference Occurred; Impact Uncertain (80% credible)**
   - Evidence: Declassified ICA, multiple T2 analyses confirm activity
   - Uncertainty: Actual causal impact on election outcomes remains disputed

2. **Russian Interference Occurred; Significant Impact (50% credible, among impact assessors)**
   - Evidence: Some T2 analyses argue interference affected outcome
   - Uncertainty: Methodological disputes; counterfactual outcomes hard to establish

3. **Russian Interference Occurred; Minimal Impact (40% credible, among impact assessors)**
   - Evidence: Other T2 analyses argue interference was noise vs. signal
   - Uncertainty: Competing methodological frameworks

**Confidence Distribution (Post-Declassification):**
```
"Declassified assessment (T1) confirms: (1) Russian government
directed interference, (2) Operations identified and assessed.

Impact assessment: T1 ICA explicitly states uncertainty on outcome
effects. T2 analyses divided: 50% argue significant impact, 40%
argue minimal. Most credible interpretation (70%): Interference
occurred but election impact remains uncertain due to methodological
limits. Alternative (25%): Impact was significant; 5%: Impact was
minimal."
```

### Key Guardrails for Intelligence Domain

1. **Temporal Anchoring Required:** "As of [year], declassified assessment stated..." (allows for updates)
2. **No Partisan Inference:** Avoid language that suggests political motivation without T1/T2 evidence
3. **Distinguish Institutional Secrecy from Suppression:** Acknowledge why records stay sealed without assuming cover-up
4. **Language Precision:** Use "confirmed" only for declassified T1; "alleged" for unverified claims
5. **Respect Competing Interpretations:** Multiple mainstream academic views deserve equal weight

---

## Domain 3: UAP/UFO/USO

### Scope & Overview

This domain covers:
- Unidentified Aerial Phenomena (official DoD term post-2022)
- Declassified military sensor data (radar, infrared, video)
- Kinematics analysis and physics constraints
- Pilot testimony and eyewitness accounts
- Institutional responses to UAP reports and investigation

### Why This Domain Matters

UAP involves:
- Significant evidence gaps (many incidents remain undeclassified or sealed)
- Multiple plausible hypotheses (sensor artifacts, classified vehicles, unknown objects)
- Physics constraints are binding (all hypotheses must satisfy known physics)
- Popular interest and speculation (risk of conflating unexplained with extraordinary)
- Recent institutional acknowledgment (2022 AARO, Congressional hearings)

### T1 Priority Sources

**Declassified Military Records:**
- AARO (All-domain Anomaly Resolution Office) public reports
- Congressional hearing records (open sessions on UAP)
- Naval Incident Response accounts (declassified portions)
- Declassified radar/sensor data (with technical specifications)
- Pilot debriefs (if declassified or authorized for discussion)

**Declassified Documentation:**
- Navy "Gimbal" and "GoFast" video metadata
- Radar logs with resolution and false positive rates
- Official incident reports with dates and locations

**T1 Citation Standard for UAP Domain:**
```
[Source: Declassified DoD incident report, "[Incident Name]",
[Date], reference [ID], available at [official AARO archive or
Congressional Record]]

or

[Source: Declassified military sensor data: [Radar/Infrared/Video],
[Date], Technical specs: [resolution/range/false positive rate]]
```

### T2 Priority Sources

**Physics & Engineering Analysis:**
- Peer-reviewed physics journals analyzing kinematics constraints
- Engineering papers on sensor capability and error rates
- Peer-reviewed studies on UAP incidents (limited but growing field)
- Academic aerodynamicist papers on propulsion constraints

**Institutional Analysis:**
- Think tank reports on DoD UAP response (RAND, etc.)
- Academic papers on institutional response to unexplained phenomena

### Key Methodology: UAP Case Study

#### Case: Gimbal Incident (Declassified Video, Date)

**Question:** What can physics constraints tell us about the Gimbal video kinematics?

**T1 Declassified Data:**

- ✅ Video declassified [year]; released by Pentagon [date]
- ✅ Recorded by: Military sensor system [type] aboard [platform]
- ✅ Reported characteristics: Speed [X mph], turning radius [Y feet], acceleration [Z Gs]
- ✅ Sensor specifications: Resolution [pixel/degree], range [nautical miles], false positive rate [X%]
- ❌ Remains classified: Full technical analysis by DoD; broader incident context
- ❌ Remains classified: Whether incident involved classified U.S. vehicle

**Sensor Quality Assessment:**

Radar/infrared system X specifications (from declassified DoD manual):
- Resolution: [Can resolve objects down to Y size]
- False positive rate: [X%] under normal conditions, up to [Y%] in atmospheric conditions
- Known failure modes: [atmospheric effects, sensor artifacts, calibration drift]

**Physics Constraints Analysis (T2 Peer-Reviewed):**

Physics paper analyzing Gimbal kinematics:
- Reported acceleration: [X Gs]
- Maximum human-survivable: [Y Gs]
- Conclusion: If object is crewed, pilot sustains [Z Gs]; possible but extreme
- Alternative: Sensor artifact from [specific atmospheric condition] with [likelihood estimate]
- Propulsion implications: [Energy requirements for reported acceleration]

**Competing Hypotheses (Ranked by Physics Plausibility):**

1. **Sensor Artifact (50% credible)**
   - Explanation: Atmospheric reflection, radar clutter, or processing error creates apparent kinematics
   - Physics: Consistent with known sensor failure modes
   - Evidence: Sensor specs allow for false positives; similar artifacts documented
   - Uncertainty: Would require detailed sensor analysis to confirm

2. **Classified U.S. Military Vehicle (30% credible)**
   - Explanation: Experimental aircraft or advanced platform under test
   - Physics: Consistent with known propulsion; extreme G-forces plausible for unmanned vehicle
   - Evidence: U.S. has history of classified aerospace platforms
   - Uncertainty: No public confirmation; would violate OPSEC to confirm

3. **Unconventional Vehicle (15% credible)**
   - Explanation: Technology using propulsion method unknown to public
   - Physics: Consistent with reported kinematics; mechanism uncertain
   - Evidence: Unexplained does not mean impossible; physics allows for alternatives
   - Uncertainty: No direct evidence; speculative

4. **Extraterrestrial Origin (5% credible)**
   - Explanation: Non-human technology from external source
   - Physics: Consistent with reported kinematics; propulsion method unknown
   - Evidence: No direct evidence; requires extraordinary claim standard (higher burden)
   - Uncertainty: Highest-magnitude claim; lowest prior probability

**Confidence Distribution:**
```
"Most likely explanation (50%): Sensor artifact or atmospheric
effect creates apparent kinematics. Physics allows this; sensor
specs support plausibility.

Alternative (30%): Classified U.S. vehicle or platform. Physics
consistent; institutional secrecy plausible.

Other explanations (20%): Unconventional vehicle (15%) or
extraordinary origin (5%). Physics permits; evidence insufficient.

Reasoning: Bayesian framework weights by prior probability and
evidence support. Sensor artifact is most likely because (1) known
failure mode, (2) most parsimonious explanation, (3) doesn't require
extraordinary claims. New declassification would likely shift
confidence by clarifying sensor quality and other incidents'
patterns."
```

### Key Guardrails for UAP Domain

1. **Physics Constraints Binding:** All hypotheses must satisfy known physics
2. **Sensor Quality Assessment Mandatory:** Evaluate resolution, false positive rates, known artifacts
3. **Witness Testimony Flagged as T4:** Credible pilots are not sensors; corroborate with T1 data
4. **Unexplained ≠ Extraterrestrial:** Hierarchy of hypotheses weighted by evidence, not exoticism
5. **Distinguish Unconventional from Impossible:** Unconventional ≠ physics-violating

---

## Domain 4: 3i_atlas_core (Astronomy & Astrophysics)

### Scope & Overview

This domain covers:
- Astronomical observations and celestial phenomena
- Space mission data and announcements
- Astrophysical analysis and physics constraints
- Observational limits and instrumental capabilities
- Competing explanations for observed phenomena

### T1 Priority Sources

**Highest-confidence sources:**
- NASA-JPL official mission data and press releases
- International space agency announcements (ESA, JAXA, CNSA)
- Peer-reviewed primary source data from major observatories
- Published astronomical survey catalogs with verifiable provenance
- Official space agency technical documentation

**Example citations:**
- NASA-JPL Exoplanet Archive (NASA primary data)
- ESA Gaia Mission data releases (peer-reviewed primary)
- Sloan Digital Sky Survey (SDSS) published catalogs

### T2 Priority Sources

- Peer-reviewed astrophysics journals (ApJ, MNRAS, A&A)
- Published research with reproducible methodologies
- Think tank analyses of space policy and astronomy

### Key Methodology: Observable Data with Physics Constraints

#### Step 1: Establish T1 Observational Baseline
- What does the official NASA-JPL or space agency data show?
- What are the raw measurements (with uncertainty ranges)?
- What instrumental specifications apply?

#### Step 2: Assess Observational Limits
- Telescope resolution and detection sensitivity
- False positive rates under different conditions
- Instrumental artifacts and known limitations

**Example:**
```
"Hubble Space Telescope detects [object] with:
- Resolution: [X arcseconds] at wavelength [Y]
- Signal-to-noise ratio: [Z]
- False positive rate under these conditions: [A%]"
```

#### Step 3: Apply Physics Constraints
- What does astrophysics permit or forbid?
- Energy requirements for observed phenomena
- Stellar/galactic evolution constraints

#### Step 4: Competing Explanations (Conventional → Exotic)
- Hypothesis A (most conventional): [astrophysical explanation], [evidence]
- Hypothesis B (alternative): [less conventional], [evidence]
- Hypothesis C (exotic): [requires paradigm shift], [minimal evidence]

#### Step 5: Confidence Quantification
```
"Most plausible (75%): [Conventional astrophysical explanation],
supported by physics constraints and observational data.

Alternative (20%): [Less conventional], would require [additional
assumptions], some conflict with standard astrophysics.

Other (5%): [Exotic hypothesis], requires extraordinary evidence
not yet provided."
```

### Domain-Specific Guardrails

1. **Physics constraints are binding:** All hypotheses must satisfy astrophysics principles
2. **Data quality assessment mandatory:** Evaluate instrument specs, resolution, false positive rates
3. **Distinguish data types:** Confirmed data vs. processed vs. interpolated
4. **Conventional first:** Present standard astrophysical explanations before exotic ones
5. **Confidence scaling:** Extraordinary claims require proportionally stronger evidence

### Case Study Methodology: Exoplanet Classification

**Question:** How do we classify an observed exoplanet and assess alternative scenarios?

**T1 Baseline:**
- Official NASA-JPL exoplanet database entry: [specific parameters]
- Detection method and uncertainty: [spectroscopy/transit/direct imaging]
- Orbital and physical characteristics: [measured values]

**T2 Analysis:**
- Peer-reviewed study of similar exoplanets: [comparison]
- Formation models and stellar evolutionary constraints: [theoretical framework]

**Physics Constraints:**
- Habitable zone calculations: [boundary conditions]
- Stellar radiation and planetary atmosphere stability: [physics limits]

**Competing Hypotheses:**
- Terrestrial planet (Earth-like) — most consistent with data
- Super-Earth/mini-Neptune — plausible alternative
- Brown dwarf or stellar companion — lower probability

**Confidence Distribution:**
```
Terrestrial classification: 70% (matches observed parameters
and formation models)
Super-Earth alternative: 25% (possible, requires different
composition assumptions)
Other: 5% (requires additional data to assess)
```

### Key Principles

- **Competing Hypotheses Valued:** Multiple astrophysical explanations coexist
- **Uncertainty Embraced:** "Needs more data" is honest scientific answer
- **Physics Binding:** Extraordinary astronomical claims require extraordinary evidence
- **Transparent Reasoning:** Show how physics constrains plausibility

---

## Summary & Cross-Domain Principles

All domains follow these consistent principles:

1. **T1 Baseline First:** Establish declassified facts before interpretation
2. **T2 Analysis Second:** Layer peer-reviewed interpretation and analysis
3. **Competing Hypotheses:** Present multiple plausible explanations with confidence
4. **Physics/Historical Constraints:** Bound plausibility by what's physically/historically possible
5. **Sealed Records Respected:** Explain why remaining sealed without assuming content
6. **Uncertainty Quantified:** Use Bayesian framing; avoid hedging language

---

**Version:** Part of BordneAI Research Engine v3.0-alpha
**Last Updated:** 2025-11-17
**License:** CC BY 4.0
**Related Files:** GOVERNANCE.md, SOURCING_PROFILE_V2.1.md, docs/examples.md
